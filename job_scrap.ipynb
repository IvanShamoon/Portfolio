{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LinkedIn Data Analyst Job Search\n",
    "\n",
    "This is a personal project made to improve my web scraping abilities using the an automated bot through the selenium python libaray. \n",
    "\n",
    "In addition, it was made to survey the data science/analyst landscape as required by employers as advertised on LinkedIn. I hoped to guaged the potential programming languages and tools which dominate the space. Because this is based on Australian listings, I hope to also visualise geographical results. What do employers expect? Is Python or R most required? These are the questions we as data science students tend to ask ourselves. This tool perhaps provides some light into the pool of uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time \n",
    "from selenium.webdriver.common.by import By\n",
    "from statistics import mode\n",
    "from collections import Counter\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "import pandas as pd\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ivan Shamoon\\AppData\\Local\\Temp\\ipykernel_10756\\2930643640.py:12: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(executable_path = PATH)\n"
     ]
    }
   ],
   "source": [
    "# Specify path file location and service\n",
    "\n",
    "PATH = \"C:\\\\Users\\\\Ivan Shamoon\\\\Desktop\\\\chromedriver.exe\"\n",
    "url = 'https://au.linkedin.com/jobs/data-analyst-jobs?position=1&pageNum=0'\n",
    "\n",
    " # Set up driver through the driver located in the path. \n",
    " # The driver is set up to lauch a proxy web browser when ran.\n",
    " # The driver then must be given website input to lauch towards a desired location.\n",
    " # Expand window size for full website capabilities\n",
    "\n",
    "driver = webdriver.Chrome(executable_path = PATH)\n",
    "driver.get(url)\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Function\n",
    "\n",
    "stop_words_clean: remove stop words and punctuation from the job descriptions\n",
    "\n",
    "augment_tool: encoder for inputed list or string. ie input 'sql' inspect whether current description contains the word 'sql': 1 if yes 0 if no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool that records the occurance of inputed lists and determines for each row if each element in the\n",
    "# Inputed list occurs in the dataframe's row.\n",
    "\n",
    "def augment_tool(df, tools):\n",
    "    df[tools] = 'NA'\n",
    "    for item in tools:\n",
    "        index = 0\n",
    "        for desc in df['job_desc']:\n",
    "            if item in desc.lower().split(' '):\n",
    "                df[item][index] = 1\n",
    "            else:\n",
    "                df[item][index] = 0\n",
    "            index = index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outputs a dictionary for each element in inputed data where stopwords are removed.\n",
    "# The returned values are split\n",
    "\n",
    "def stop_words_clean(job_desc):\n",
    "\n",
    "    words = job_desc\n",
    "    filtered_sentences = {}\n",
    "    index = 0\n",
    "\n",
    "    for synopsis in job_desc:\n",
    "\n",
    "        # Initialise with nlp    \n",
    "        my_doc = nlp(synopsis.lower())\n",
    "        token_list = []\n",
    "        \n",
    "        # Convert all words in each synopsis to text\n",
    "        for word in my_doc:\n",
    "            token_list.append(word.text)\n",
    "\n",
    "        # For each word in a synopsis, determine whether current element is a stopword\n",
    "        # If not, append word into dictionary keyed by index\n",
    "        for word in token_list:\n",
    "            lexeme = nlp.vocab[word]\n",
    "\n",
    "            if index not in filtered_sentences:\n",
    "                if lexeme.is_stop == False:\n",
    "                    filtered_sentences[index] = [word]\n",
    "\n",
    "            else:\n",
    "                if lexeme.is_stop == False:\n",
    "                    filtered_sentences[index].append(word)\n",
    "\n",
    "        # Move to next synopsis\n",
    "        index = index + 1\n",
    "\n",
    "    # Join each element in the dictionary whose entries are separated\n",
    "    for key in filtered_sentences:\n",
    "        filtered_sentences[key] = ' '.join(filtered_sentences[key]) \n",
    "    \n",
    "    # Expand dictionary entries into a list and remove punctuation\n",
    "    filtered_sentences = pd.Series(filtered_sentences)\n",
    "    filtered_sentences = filtered_sentences.str.replace(r'[^\\w\\s]+', '')\n",
    "\n",
    "    # Remove last paragraph (it is irrelavent)\n",
    "    loop = []\n",
    "    index = 0\n",
    "    for item in filtered_sentences:\n",
    "        element = item.split('\\n\\n')\n",
    "        length = len(element)\n",
    "        filtered_sentences[index] = element[0:length-1]\n",
    "        index = index + 1\n",
    "\n",
    "    index = 0\n",
    "    for item in filtered_sentences:\n",
    "        filtered_sentences[index] = ' '.join(filtered_sentences[index])\n",
    "        index = index + 1\n",
    " \n",
    "    return filtered_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issues that needed to be addressed\n",
    "\n",
    "Unlike static pages where the web sacrping tool BeautifulSoup excels in, to gather the most information and to utilize the clicking options offered by LinkedIn, a most robust tool in the form of Selenium must be used. \n",
    "\n",
    "The page layout of LinkedIn job listings is not static. It requires the user to individually click on listings on the side-board for a pop up with specific information about the job to show up. Three things to note about this:\n",
    "\n",
    "* The layout can only be accessed when the window size is large enough to offer the information. This is why the driver's window is set to the maximum size\n",
    "* There needs to be a wait period when clicking due to internet buffering or slowness\n",
    "* What happens when gitches in the website occurs and no pop up shows despite clicking on it. The try/except functions helps to pass through the loop when this occurs. Therefore the possibility of errors cannot hault the code from running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_block = driver.find_element(By.CLASS_NAME, 'two-pane-serp-page__results-list')\n",
    "\n",
    "for i in range(0,7):\n",
    "    driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "    time.sleep(2)\n",
    "\n",
    "for i in range(0,10):\n",
    "    more_job_button = main_block.find_element(By.TAG_NAME, 'button')\n",
    "    more_job_button.click()\n",
    "    time.sleep(3)\n",
    "    driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_two = 0\n",
    "job_desc = []\n",
    "job_title = []\n",
    "job_indus = []\n",
    "job_location = []\n",
    "job_employ = []\n",
    "\n",
    "# Identifies main block for job listings\n",
    "main_block = driver.find_element(By.CLASS_NAME, 'two-pane-serp-page__results-list')\n",
    "\n",
    "# Select all individual blocks from job listing\n",
    "job_block = main_block.find_elements(By.TAG_NAME, 'li')\n",
    "\n",
    "# Go through each item in the element\n",
    "for index in range(0,len(job_block)):\n",
    "\n",
    "    # Identify block\n",
    "    block = job_block[index]\n",
    "\n",
    "    # Clicks the job and opens the side bar avaliability\n",
    "    block.click()\n",
    "\n",
    "    # Wait for contents to completely load\n",
    "    time.sleep(2)\n",
    "\n",
    "    try:\n",
    "        # Top portion of select job option\n",
    "        top_card = driver.find_element(By.CLASS_NAME, 'two-pane-serp-page__detail-view')\n",
    "\n",
    "        # With loaded contents select the side bar element\n",
    "        show_more = driver.find_element(By.CLASS_NAME,'decorated-job-posting__details')\n",
    "\n",
    "        # Get Title    \n",
    "        title = top_card.find_element(By.CLASS_NAME, 'topcard__link')\n",
    "\n",
    "        # Get location\n",
    "        location = top_card.find_element(By.XPATH, \"//span[@class = 'topcard__flavor topcard__flavor--bullet']\")\n",
    "\n",
    "        # Get bottom card \n",
    "        bottom_card = top_card.find_elements(By.XPATH, \"//span[@class = 'description__job-criteria-text description__job-criteria-text--criteria']\")\n",
    "            \n",
    "        # Get industry \n",
    "        industry = bottom_card[3]\n",
    "\n",
    "        # Get employment type\n",
    "        employment = bottom_card[1]\n",
    "            \n",
    "        # From side bar element click the show more button for full description\n",
    "        show_more_button = show_more.find_element(By.TAG_NAME, 'button')\n",
    "        show_more_button.click()\n",
    "\n",
    "        # Wait for loaded contents\n",
    "        time.sleep(2)\n",
    "\n",
    "        # Append items\n",
    "        job_desc.append(show_more.text)\n",
    "        job_title.append(title.text)\n",
    "        job_indus.append(industry.text)\n",
    "        job_location.append(location.text)\n",
    "        job_employ.append(employment.text)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'job_title': job_title,\n",
    "                    'job_desc': job_desc,\n",
    "                    'job_location': job_location,\n",
    "                    'job_indus': job_indus,\n",
    "                    'employ_type': job_employ})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Data and Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ivan Shamoon\\AppData\\Local\\Temp\\ipykernel_10756\\264959438.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['job_desc'][index] = element\n",
      "C:\\Users\\Ivan Shamoon\\AppData\\Local\\Temp\\ipykernel_10756\\264959438.py:12: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data['job_desc'] = data['job_desc'].str.replace(r'[^\\w\\s]+', '')\n"
     ]
    }
   ],
   "source": [
    "# Strips \\n element from each job description.\n",
    "# Removes punctuation from each element for future word identification easibility\n",
    "index = 0\n",
    "for item in data['job_desc']:\n",
    "    element = item.split('\\n')\n",
    "    element = list(map(str.strip, element))\n",
    "    element = ' '.join(element)\n",
    "    data['job_desc'][index] = element\n",
    "    index = index + 1\n",
    "\n",
    "data['job_desc'] = data['job_desc'].str.replace(r'[^\\w\\s]+', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not an exhaustive list of tools, but is information gathered from the internet and surveying a lot of job listings and determining common words, languages, and BI tools in most listings within Australia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prog_lan = ['sql','r','python','java','matlab','javascript','mongodb','nosql','mysql','c#','c++']\n",
    "data_tools = ['powerbi', 'power', 'microstrategy','looking', 'sap', 'sisense','tibco', 'qlik', 'qlikwise', 'sas','jupyter', 'agile','bigquery' , 'tableau', 'google','IBM','chartio','excel','alteryx','word','powerpoint','shiny','lambda','azure','teradata']\n",
    "\n",
    "augment_tool(data,prog_lan)\n",
    "augment_tool(data,data_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get first element from job industry\n",
    "index = 0\n",
    "for item in data['job_indus']:\n",
    "    element = item.split(',')[0]\n",
    "    data['job_indus'][index] = element\n",
    "    index = index + 1\n",
    "\n",
    "# Set column stype to string\n",
    "data['job_indus'] = data['job_indus'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because there are a lot of industries with many of them overlapping but not sematically, it is a good idea to cohort them into their general field. Otherwise many information in the visualisation will be lost in the technicality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streamline the job industries into its category\n",
    "\n",
    "for index, item in enumerate(data['job_indus']):\n",
    "    if 'retail' in item.lower():\n",
    "        data['job_indus'][index] = 'Retail'\n",
    "    elif any(word in item.lower() for word in ['pharam' ,'medic', 'biot','health']):\n",
    "        data['job_indus'][index] = 'Health'\n",
    "    elif any(word in item.lower() for word in ['insur', 'financ','inves', 'estate']):\n",
    "        data['job_indus'][index] = 'Financial Services'\n",
    "    elif any(word in item.lower() for word in ['truck', 'transp','oil', 'travel','automo','airlin']):\n",
    "        data['job_indus'][index] = 'Transportation'\n",
    "    elif any(word in item.lower() for word in ['comput','software', 'techno','applianc']):\n",
    "        data['job_indus'][index] = 'Software Development'\n",
    "    elif any(word in item.lower() for word in ['engineering','manufa','construc','mining','building','build','environ']):\n",
    "        data['job_indus'][index] = 'Engineering'\n",
    "    elif any(word in item.lower() for word in ['legal','govern']):\n",
    "        data['job_indus'][index] = 'Government'\n",
    "    elif any(word in item.lower() for word in ['telecom','adver','entertain', 'media','communi']):\n",
    "        data['job_indus'][index] = 'Advertisement'\n",
    "    elif any(word in item.lower() for word in ['human','education','e-learn','public','social']):\n",
    "        data['job_indus'][index] = 'Human Services'\n",
    "    elif any(word in item.lower() for word in ['it', 'consulting','resear']):\n",
    "        data['job_indus'][index] = 'IT Consulting'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the location and designate them into the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to retreive second element from job location column as the majority of the states are found in there\n",
    "# Else retreive the whole element\n",
    "capital =  ['sydney', 'melbourne', 'hobart','brisbane','adelaide','perth']\n",
    "state = ['New South Wales',' Victoria','Tasmania','Queesland','South Australia','Western Australia']\n",
    "\n",
    "for index, item in enumerate(data['job_location']):\n",
    "    try:\n",
    "        data['job_location'][index] = item.split(',')[1].strip()\n",
    "    except:\n",
    "        if capital[0] in item.lower():\n",
    "            data['job_location'][index] = state[0]\n",
    "        elif capital[1] in item.lower():\n",
    "            data['job_location'][index] = state[1]\n",
    "        elif capital[2] in item.lower():\n",
    "            data['job_location'][index] = state[2]\n",
    "        elif capital[3] in item.lower():\n",
    "            data['job_location'][index] = state[3]\n",
    "        elif capital[4] in item.lower():\n",
    "            data['job_location'][index] = state[4]\n",
    "        elif capital[5] in item.lower():\n",
    "            data['job_location'][index] = state[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['job_location'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('linked_scrap.csv')\n",
    "data = pd.read_csv('linkedin_scrap.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prog_lan = ['sql','r','python','java','matlab','javascript','mongodb','nosql','mysql','c#','c++']\n",
    "data_tools = ['powerbi', 'power', 'microstrategy', 'sap', 'sisense','tibco', 'qlik', 'qlikwise', 'sas','jupyter', 'agile','bigquery' , 'tableau', 'google','IBM','chartio','excel','alteryx','word','powerpoint','shiny','lambda','azure','teradata']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sql           256\n",
       "r              77\n",
       "python        118\n",
       "java           11\n",
       "matlab          1\n",
       "javascript      6\n",
       "mongodb         2\n",
       "nosql           6\n",
       "mysql           1\n",
       "c#              0\n",
       "c++             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[prog_lan].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "powerbi           36\n",
       "power            148\n",
       "microstrategy      4\n",
       "sap               19\n",
       "sisense            1\n",
       "tibco              0\n",
       "qlik              20\n",
       "qlikwise           0\n",
       "sas               24\n",
       "jupyter            4\n",
       "agile             76\n",
       "bigquery          14\n",
       "tableau           97\n",
       "google            44\n",
       "IBM                0\n",
       "chartio            0\n",
       "excel            115\n",
       "alteryx           27\n",
       "word              15\n",
       "powerpoint        11\n",
       "shiny              2\n",
       "lambda             3\n",
       "azure             58\n",
       "teradata          18\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data_tools].sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d945ed2a52083c4a5b72234668779b0e7ade9860b23e6b7df5d4937d091dd521"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
